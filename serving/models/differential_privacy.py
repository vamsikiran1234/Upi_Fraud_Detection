"""
Differential Privacy for Sensitive Feature Protection
Protects individual privacy while maintaining model utility
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import math
import random
from datetime import datetime
import logging

@dataclass
class PrivacyBudget:
    """Privacy budget for differential privacy"""
    epsilon: float  # Privacy parameter
    delta: float    # Failure probability
    total_epsilon: float = 0.0
    total_delta: float = 0.0
    remaining_epsilon: float = 0.0
    remaining_delta: float = 0.0

class LaplaceMechanism:
    """Laplace mechanism for differential privacy"""
    
    def __init__(self, epsilon: float, sensitivity: float = 1.0):
        self.epsilon = epsilon
        self.sensitivity = sensitivity
    
    def add_noise(self, data: np.ndarray) -> np.ndarray:
        """Add Laplace noise to data"""
        # Calculate noise scale
        scale = self.sensitivity / self.epsilon
        
        # Generate Laplace noise
        noise = np.random.laplace(0, scale, data.shape)
        
        return data + noise
    
    def add_noise_to_query(self, query_result: float, sensitivity: float = None) -> float:
        """Add noise to a single query result"""
        if sensitivity is None:
            sensitivity = self.sensitivity
        
        scale = sensitivity / self.epsilon
        noise = np.random.laplace(0, scale)
        
        return query_result + noise

class GaussianMechanism:
    """Gaussian mechanism for differential privacy"""
    
    def __init__(self, epsilon: float, delta: float, sensitivity: float = 1.0):
        self.epsilon = epsilon
        self.delta = delta
        self.sensitivity = sensitivity
    
    def add_noise(self, data: np.ndarray) -> np.ndarray:
        """Add Gaussian noise to data"""
        # Calculate noise scale
        scale = self.sensitivity * math.sqrt(2 * math.log(1.25 / self.delta)) / self.epsilon
        
        # Generate Gaussian noise
        noise = np.random.normal(0, scale, data.shape)
        
        return data + noise
    
    def add_noise_to_query(self, query_result: float, sensitivity: float = None) -> float:
        """Add noise to a single query result"""
        if sensitivity is None:
            sensitivity = self.sensitivity
        
        scale = sensitivity * math.sqrt(2 * math.log(1.25 / self.delta)) / self.epsilon
        noise = np.random.normal(0, scale)
        
        return query_result + noise

class ExponentialMechanism:
    """Exponential mechanism for differential privacy"""
    
    def __init__(self, epsilon: float, sensitivity: float = 1.0):
        self.epsilon = epsilon
        self.sensitivity = sensitivity
    
    def select_with_privacy(self, 
                          candidates: List[Any], 
                          utility_function: callable,
                          data: Any) -> Any:
        """Select candidate with differential privacy"""
        # Calculate utility scores
        utility_scores = [utility_function(candidate, data) for candidate in candidates]
        
        # Calculate probabilities using exponential mechanism
        probabilities = []
        for score in utility_scores:
            prob = math.exp(self.epsilon * score / (2 * self.sensitivity))
            probabilities.append(prob)
        
        # Normalize probabilities
        total_prob = sum(probabilities)
        probabilities = [p / total_prob for p in probabilities]
        
        # Select candidate based on probabilities
        selected_idx = np.random.choice(len(candidates), p=probabilities)
        
        return candidates[selected_idx]

class PrivateFeatureExtractor:
    """Extract features with differential privacy"""
    
    def __init__(self, privacy_budget: PrivacyBudget):
        self.privacy_budget = privacy_budget
        self.laplace_mechanism = LaplaceMechanism(privacy_budget.epsilon)
        self.gaussian_mechanism = GaussianMechanism(
            privacy_budget.epsilon, 
            privacy_budget.delta
        )
    
    def extract_private_statistics(self, 
                                 data: pd.DataFrame, 
                                 sensitive_columns: List[str]) -> Dict[str, Any]:
        """Extract statistics with differential privacy"""
        private_stats = {}
        
        for column in sensitive_columns:
            if column not in data.columns:
                continue
            
            column_data = data[column].dropna()
            
            # Calculate sensitivity (assuming bounded data)
            sensitivity = self._calculate_sensitivity(column_data)
            
            # Extract private statistics
            private_stats[column] = {
                'mean': self._private_mean(column_data, sensitivity),
                'std': self._private_std(column_data, sensitivity),
                'count': self._private_count(column_data),
                'min': self._private_min(column_data, sensitivity),
                'max': self._private_max(column_data, sensitivity)
            }
        
        return private_stats
    
    def _calculate_sensitivity(self, data: pd.Series) -> float:
        """Calculate sensitivity for a data column"""
        # For bounded data, sensitivity is the range
        if data.dtype in ['int64', 'float64']:
            return float(data.max() - data.min())
        else:
            # For categorical data, sensitivity is 1
            return 1.0
    
    def _private_mean(self, data: pd.Series, sensitivity: float) -> float:
        """Calculate private mean"""
        true_mean = data.mean()
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        return true_mean + noise
    
    def _private_std(self, data: pd.Series, sensitivity: float) -> float:
        """Calculate private standard deviation"""
        true_std = data.std()
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        return max(0, true_std + noise)  # Ensure non-negative
    
    def _private_count(self, data: pd.Series) -> int:
        """Calculate private count"""
        true_count = len(data)
        noise = np.random.laplace(0, 1.0 / self.privacy_budget.epsilon)
        return max(0, int(true_count + noise))
    
    def _private_min(self, data: pd.Series, sensitivity: float) -> float:
        """Calculate private minimum"""
        true_min = data.min()
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        return true_min + noise
    
    def _private_max(self, data: pd.Series, sensitivity: float) -> float:
        """Calculate private maximum"""
        true_max = data.max()
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        return true_max + noise

class PrivateMLModel(nn.Module):
    """Differentially private machine learning model"""
    
    def __init__(self, 
                 input_dim: int, 
                 hidden_dim: int = 64, 
                 output_dim: int = 2,
                 privacy_budget: PrivacyBudget = None):
        super(PrivateMLModel, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.privacy_budget = privacy_budget or PrivacyBudget(epsilon=1.0, delta=1e-5)
        
        # Model architecture
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # Privacy parameters
        self.noise_multiplier = 1.1  # Noise multiplier for DP-SGD
        self.max_grad_norm = 1.0     # Gradient clipping norm
        self.learning_rate = 0.01
    
    def forward(self, x):
        return self.network(x)
    
    def private_training_step(self, 
                            batch_x: torch.Tensor, 
                            batch_y: torch.Tensor,
                            optimizer: optim.Optimizer) -> Dict[str, float]:
        """Perform one private training step"""
        # Forward pass
        outputs = self.forward(batch_x)
        loss = nn.CrossEntropyLoss()(outputs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(self.parameters(), self.max_grad_norm)
        
        # Add noise to gradients
        self._add_noise_to_gradients()
        
        # Update parameters
        optimizer.step()
        
        return {
            'loss': loss.item(),
            'privacy_cost': self._calculate_privacy_cost()
        }
    
    def _add_noise_to_gradients(self):
        """Add noise to gradients for differential privacy"""
        for param in self.parameters():
            if param.grad is not None:
                # Calculate noise scale
                noise_scale = self.noise_multiplier * self.max_grad_norm
                
                # Add Gaussian noise
                noise = torch.normal(0, noise_scale, param.grad.shape)
                param.grad += noise
    
    def _calculate_privacy_cost(self) -> float:
        """Calculate privacy cost of this training step"""
        # Simplified privacy cost calculation
        # In practice, use more sophisticated methods like RDP
        return self.noise_multiplier * self.max_grad_norm

class DPQueryProcessor:
    """Process queries with differential privacy"""
    
    def __init__(self, privacy_budget: PrivacyBudget):
        self.privacy_budget = privacy_budget
        self.query_history = []
        self.total_privacy_cost = 0.0
    
    def private_count_query(self, 
                          data: pd.DataFrame, 
                          condition: str = None) -> int:
        """Execute private count query"""
        if condition:
            filtered_data = data.query(condition)
        else:
            filtered_data = data
        
        true_count = len(filtered_data)
        
        # Add Laplace noise
        noise = np.random.laplace(0, 1.0 / self.privacy_budget.epsilon)
        private_count = max(0, int(true_count + noise))
        
        # Track privacy cost
        self._track_privacy_cost(1.0)
        
        return private_count
    
    def private_sum_query(self, 
                         data: pd.DataFrame, 
                         column: str,
                         condition: str = None) -> float:
        """Execute private sum query"""
        if condition:
            filtered_data = data.query(condition)
        else:
            filtered_data = data
        
        true_sum = filtered_data[column].sum()
        
        # Calculate sensitivity (assuming bounded data)
        sensitivity = self._calculate_sum_sensitivity(data[column])
        
        # Add Laplace noise
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        private_sum = true_sum + noise
        
        # Track privacy cost
        self._track_privacy_cost(sensitivity)
        
        return private_sum
    
    def private_average_query(self, 
                            data: pd.DataFrame, 
                            column: str,
                            condition: str = None) -> float:
        """Execute private average query"""
        if condition:
            filtered_data = data.query(condition)
        else:
            filtered_data = data
        
        true_avg = filtered_data[column].mean()
        
        # Calculate sensitivity for average
        sensitivity = self._calculate_avg_sensitivity(data[column])
        
        # Add Laplace noise
        noise = np.random.laplace(0, sensitivity / self.privacy_budget.epsilon)
        private_avg = true_avg + noise
        
        # Track privacy cost
        self._track_privacy_cost(sensitivity)
        
        return private_avg
    
    def _calculate_sum_sensitivity(self, data: pd.Series) -> float:
        """Calculate sensitivity for sum query"""
        return float(data.max() - data.min())
    
    def _calculate_avg_sensitivity(self, data: pd.Series) -> float:
        """Calculate sensitivity for average query"""
        return float(data.max() - data.min()) / len(data)
    
    def _track_privacy_cost(self, cost: float):
        """Track privacy cost of queries"""
        self.total_privacy_cost += cost
        self.query_history.append({
            'timestamp': datetime.utcnow(),
            'privacy_cost': cost,
            'total_cost': self.total_privacy_cost
        })
    
    def get_privacy_status(self) -> Dict[str, Any]:
        """Get current privacy status"""
        return {
            'total_privacy_cost': self.total_privacy_cost,
            'remaining_budget': self.privacy_budget.epsilon - self.total_privacy_cost,
            'budget_utilization': self.total_privacy_cost / self.privacy_budget.epsilon,
            'query_count': len(self.query_history),
            'can_make_queries': self.total_privacy_cost < self.privacy_budget.epsilon
        }

class DifferentialPrivacyAPI:
    """API for differential privacy operations"""
    
    def __init__(self, epsilon: float = 1.0, delta: float = 1e-5):
        self.privacy_budget = PrivacyBudget(epsilon=epsilon, delta=delta)
        self.feature_extractor = PrivateFeatureExtractor(self.privacy_budget)
        self.query_processor = DPQueryProcessor(self.privacy_budget)
        self.private_model = None
    
    def anonymize_dataset(self, 
                         data: pd.DataFrame, 
                         sensitive_columns: List[str],
                         k_anonymity: int = 3) -> pd.DataFrame:
        """Apply k-anonymity to dataset"""
        anonymized_data = data.copy()
        
        # Group by quasi-identifiers
        quasi_identifiers = [col for col in data.columns if col not in sensitive_columns]
        
        if quasi_identifiers:
            # Apply k-anonymity
            groups = anonymized_data.groupby(quasi_identifiers)
            
            for name, group in groups:
                if len(group) < k_anonymity:
                    # Suppress or generalize small groups
                    anonymized_data = anonymized_data.drop(group.index)
        
        return anonymized_data
    
    def add_differential_privacy_noise(self, 
                                     data: np.ndarray, 
                                     mechanism: str = 'laplace') -> np.ndarray:
        """Add differential privacy noise to data"""
        if mechanism == 'laplace':
            mechanism_obj = LaplaceMechanism(self.privacy_budget.epsilon)
        elif mechanism == 'gaussian':
            mechanism_obj = GaussianMechanism(
                self.privacy_budget.epsilon, 
                self.privacy_budget.delta
            )
        else:
            raise ValueError(f"Unknown mechanism: {mechanism}")
        
        return mechanism_obj.add_noise(data)
    
    def train_private_model(self, 
                          X: np.ndarray, 
                          y: np.ndarray,
                          epochs: int = 100,
                          batch_size: int = 32) -> Dict[str, Any]:
        """Train a differentially private model"""
        print("ðŸ”’ Training Differentially Private Model...")
        
        # Initialize model
        input_dim = X.shape[1]
        output_dim = len(np.unique(y))
        
        self.private_model = PrivateMLModel(
            input_dim=input_dim,
            output_dim=output_dim,
            privacy_budget=self.privacy_budget
        )
        
        # Training setup
        optimizer = optim.Adam(self.private_model.parameters(), lr=0.01)
        criterion = nn.CrossEntropyLoss()
        
        # Convert to tensors
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.LongTensor(y)
        
        # Create data loader
        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Training loop
        training_history = []
        total_privacy_cost = 0.0
        
        for epoch in range(epochs):
            epoch_loss = 0
            epoch_privacy_cost = 0
            
            for batch_x, batch_y in dataloader:
                step_result = self.private_model.private_training_step(
                    batch_x, batch_y, optimizer
                )
                
                epoch_loss += step_result['loss']
                epoch_privacy_cost += step_result['privacy_cost']
            
            total_privacy_cost += epoch_privacy_cost
            
            training_history.append({
                'epoch': epoch,
                'loss': epoch_loss / len(dataloader),
                'privacy_cost': epoch_privacy_cost,
                'total_privacy_cost': total_privacy_cost
            })
            
            if epoch % 20 == 0:
                print(f"   Epoch {epoch}/{epochs} - Loss: {epoch_loss/len(dataloader):.4f}, "
                      f"Privacy Cost: {epoch_privacy_cost:.4f}")
        
        return {
            'training_history': training_history,
            'final_privacy_cost': total_privacy_cost,
            'privacy_budget_used': total_privacy_cost / self.privacy_budget.epsilon,
            'model_trained': True
        }
    
    def make_private_prediction(self, X: np.ndarray) -> Dict[str, Any]:
        """Make predictions with differential privacy"""
        if self.private_model is None:
            raise ValueError("Model must be trained before making predictions")
        
        self.private_model.eval()
        
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X)
            outputs = self.private_model(X_tensor)
            probabilities = torch.softmax(outputs, dim=1)
            
            # Add noise to predictions for additional privacy
            noise = torch.normal(0, 0.01, probabilities.shape)
            noisy_probabilities = probabilities + noise
            noisy_probabilities = torch.clamp(noisy_probabilities, 0, 1)
            
            # Normalize probabilities
            noisy_probabilities = noisy_probabilities / noisy_probabilities.sum(dim=1, keepdim=True)
            
            predictions = torch.argmax(noisy_probabilities, dim=1)
            
            return {
                'predictions': predictions.numpy(),
                'probabilities': noisy_probabilities.numpy(),
                'privacy_protected': True
            }
    
    def get_privacy_report(self) -> Dict[str, Any]:
        """Get comprehensive privacy report"""
        query_status = self.query_processor.get_privacy_status()
        
        return {
            'privacy_budget': {
                'epsilon': self.privacy_budget.epsilon,
                'delta': self.privacy_budget.delta,
                'total_used': query_status['total_privacy_cost'],
                'remaining': query_status['remaining_budget'],
                'utilization': query_status['budget_utilization']
            },
            'query_statistics': {
                'total_queries': query_status['query_count'],
                'can_make_queries': query_status['can_make_queries']
            },
            'model_privacy': {
                'model_trained': self.private_model is not None,
                'differential_privacy_enabled': True,
                'noise_mechanism': 'Gaussian + Laplace'
            },
            'privacy_guarantees': {
                'epsilon_differential_privacy': True,
                'delta_failure_probability': self.privacy_budget.delta,
                'composition_theorem_applied': True
            }
        }

# Example usage and testing
def demonstrate_differential_privacy():
    """Demonstrate differential privacy system"""
    print("ðŸ”’ Demonstrating Differential Privacy System")
    print("=" * 60)
    
    # Create synthetic sensitive data
    np.random.seed(42)
    n_samples = 1000
    
    data = pd.DataFrame({
        'user_id': range(n_samples),
        'age': np.random.randint(18, 80, n_samples),
        'income': np.random.lognormal(10, 0.5, n_samples),
        'transaction_amount': np.random.lognormal(6, 1, n_samples),
        'fraud': np.random.binomial(1, 0.1, n_samples)
    })
    
    print(f"Created {len(data)} samples with sensitive data")
    print(f"Fraud rate: {data['fraud'].mean():.3f}")
    
    # Initialize DP API
    dp_api = DifferentialPrivacyAPI(epsilon=1.0, delta=1e-5)
    
    # Test private feature extraction
    print("\n1. Private Feature Extraction...")
    sensitive_columns = ['age', 'income', 'transaction_amount']
    private_stats = dp_api.feature_extractor.extract_private_statistics(data, sensitive_columns)
    
    for column, stats in private_stats.items():
        print(f"   {column}:")
        print(f"     Private mean: {stats['mean']:.2f}")
        print(f"     Private std: {stats['std']:.2f}")
        print(f"     Private count: {stats['count']}")
    
    # Test private queries
    print("\n2. Private Query Processing...")
    
    # Private count query
    fraud_count = dp_api.query_processor.private_count_query(data, "fraud == 1")
    print(f"   Private fraud count: {fraud_count}")
    
    # Private sum query
    total_amount = dp_api.query_processor.private_sum_query(data, 'transaction_amount')
    print(f"   Private total amount: {total_amount:.2f}")
    
    # Private average query
    avg_income = dp_api.query_processor.private_average_query(data, 'income')
    print(f"   Private average income: {avg_income:.2f}")
    
    # Test private model training
    print("\n3. Private Model Training...")
    X = data[['age', 'income', 'transaction_amount']].values
    y = data['fraud'].values
    
    training_result = dp_api.train_private_model(X, y, epochs=50)
    print(f"   Training completed")
    print(f"   Final privacy cost: {training_result['final_privacy_cost']:.4f}")
    print(f"   Budget utilization: {training_result['privacy_budget_used']:.3f}")
    
    # Test private predictions
    print("\n4. Private Predictions...")
    test_data = X[:10]  # Test on first 10 samples
    predictions = dp_api.make_private_prediction(test_data)
    
    print(f"   Predictions: {predictions['predictions']}")
    print(f"   Privacy protected: {predictions['privacy_protected']}")
    
    # Test data anonymization
    print("\n5. Data Anonymization...")
    anonymized_data = dp_api.anonymize_dataset(data, sensitive_columns, k_anonymity=5)
    print(f"   Original data size: {len(data)}")
    print(f"   Anonymized data size: {len(anonymized_data)}")
    print(f"   Data reduction: {(1 - len(anonymized_data)/len(data))*100:.1f}%")
    
    # Get privacy report
    print("\n6. Privacy Report...")
    privacy_report = dp_api.get_privacy_report()
    
    print(f"   Privacy budget (Îµ): {privacy_report['privacy_budget']['epsilon']}")
    print(f"   Privacy budget used: {privacy_report['privacy_budget']['total_used']:.4f}")
    print(f"   Remaining budget: {privacy_report['privacy_budget']['remaining']:.4f}")
    print(f"   Budget utilization: {privacy_report['privacy_budget']['utilization']:.3f}")
    print(f"   Total queries: {privacy_report['query_statistics']['total_queries']}")
    print(f"   Can make more queries: {privacy_report['query_statistics']['can_make_queries']}")
    
    print("\nâœ… Differential Privacy demonstration completed!")
    
    return {
        'dp_api': dp_api,
        'private_stats': private_stats,
        'training_result': training_result,
        'predictions': predictions,
        'privacy_report': privacy_report
    }

if __name__ == "__main__":
    # Run demonstration
    results = demonstrate_differential_privacy()
    
    print(f"\nðŸ“Š Summary:")
    print(f"   Differential privacy system operational")
    print(f"   Privacy budget utilization: {results['privacy_report']['privacy_budget']['utilization']:.3f}")
    print(f"   Private model trained: {results['training_result']['model_trained']}")
    print(f"   Privacy guarantees: Îµ-DP with Î´={results['dp_api'].privacy_budget.delta}")
